---
title: "Depth Estimation Using"
subtitle: "Segmentation in Natural Images"
author: "Shrayan Roy, Roll No: MD2220"
institute: "Supervisor: Dr. Deepayan Sarkar"
date: "Indian Statistical Institute, Kolkata"
header-includes:
  -\usepackage{bbm}
  -\usepackage{mathtools}
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [design.css,fonts.css]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: 16:9
      
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{css,echo = FALSE}
.remark-slide-number {
  display: none;
}
```

# Introduction

* Humans possess a natural ability to perceive *3D structure* from 2D images.

* Primarily relying on *visual cues* such as perspective, relative object sizes.

```{r ,warning=FALSE,echo=FALSE,out.width='70%',fig.align='center',echo=FALSE,fig.cap= "Figure: 3D perspective from 2D image"}

knitr::include_graphics("pimg/dof.jpg")
```

---

# Depth: the third dimension

* Traditional photographs are two dimensional projections of a three dimensional scene.

* The third dimension is **depth**, which represents the distance between camera and objects in the image.

* Estimating depth of an image has interesting applications such as post-capture image refocusing and automatic scene segmentation.

--

* Most approaches involve **multiple images** of same scene or **hardware based solutions** like light emitters and coded apertures.

--

* Depth estimation from single image is more challenging, as we have only one observation per pixel.

* In this project we aim to estimate depth given a single image of the scene.

---

# Depth from Defocus

* Depth estimation from defocus blur exploits the phenomenon where objects appear more blurred depending on their distance from the camera lens.

* Measuring level of blur for each pixel can be used as a *surrogate* for depth.

```{r ,warning=FALSE,echo=FALSE,out.width='44%',fig.align='center',echo=FALSE,fig.cap= "Figure: Bluriness depending on distance from camera"}

knitr::include_graphics("pimg/depfoc.png")
```

---

# Levin et al. (2007)

* Levin et al. utilized this idea to estimate level of blur for each pixel.

* They used *sparse gradient prior* on natural images.

* But it requires a modified camera with a special coded aperture.


```{r ,warning=FALSE,echo=FALSE,out.width='75%',fig.align='center',echo=FALSE,fig.cap= "Figure: Levin et al. 2007 - Image and Depth from a Conventional Camera with a Coded Aperture"}

knitr::include_graphics("pimg/levin.png")
```

---

# Zhu et al. (2013)

* Zhu et al. employed same idea using Gabor filters for local frequency component analysis.

* But they have used *simple gradient prior* on natural images.

* Their method does not require special coded aperture.

```{r ,warning=FALSE,echo=FALSE,out.width='85%',fig.align='center',echo=FALSE,fig.cap= "Figure: Zhu et al. 2013 - Estimating Spatially Varying Defocus Blur from A Single Image"}

knitr::include_graphics("pimg/zhu.png")
```

---

# Our Approach: Main Idea

* Use of more **general prior** for image proposed by Nandy (2021).

--

* Parametric models to estimate level of blur as surrogate for depth.

--

* Instead of doing post estimation segmentation, start with pre-segmented image.

* Estimate blur (depth) for each segment separately.

* Use of Modern segmentation algorithms such as **Segment-Anything**.

---

# Point Spread Function

* When light rays spread from a point source and hit the camera lens, they should ideally refract and converge on the corresponding pixel of the original scene.

* However, if the source is out of focus, the refracted rays spread out over neighboring pixels as well.

--

* This spreading pattern is called the Point Spread Function (PSF) or Blur Kernel.

--

```{r ,warning=FALSE,echo=FALSE,out.width='65%',fig.align='center',echo=FALSE,fig.cap= "Figure: Point Spread Function"}

knitr::include_graphics("pimg/psfimg.png")
```

---

# Model for Blurred Image

* The blurred image can be viewed as **convolution** of original sharp image and Point Spread Function. 

* The observed blurred image $\boldsymbol{b}$ of dimension $M \times N$ can be modeled as - 

  $$\boldsymbol{b} = \boldsymbol{k} \ \otimes \ \boldsymbol{l} \ + \ \boldsymbol{\epsilon}$$
Where,

  * $\boldsymbol{k}$ is an $m \times n$ PSF / blur kernel.
  
  * $\boldsymbol{l}$ is the $(M + m) \times (N + n)$ *true latent image* which we want to estimate.
  
  * $\boldsymbol{\epsilon}$ is an $M \times N$ matrix of noise.
  
  * $\otimes$ denotes the *valid convolution* operator.

--

* Estimating both $\boldsymbol{l}$ and $\boldsymbol{k}$ using above model is called *blind deconvolution problem*.

--

* Number of observations $MN$ is very large compared number of parameters $\implies$ **ill-posed problem** 

--

* Can use some prior on $\boldsymbol{l}$ to deal with this ? 

---

# Model for Blurred Image


* The model defined in last slide assumes that PSF is *shift invariant* i.e. same PSF applies to all pixels.

--

* In the context of defocus blur, PSF/ Blur Kernel is *spatially varying*.

--

* We assume that $\boldsymbol{k_t}$ is shift invariant in a neighborhood ${\boldsymbol{\eta_t}}$ of size $p_1(\boldsymbol{t}) \times p_2(\boldsymbol{t})$ containing $\boldsymbol{t}$.

* Based on this assumption, our model for *spatially varying blur* is given by - 

  $$\boldsymbol{y[t']} = (\boldsymbol{k_t} \ \otimes \ \boldsymbol{x})\boldsymbol{[t']} \ + \ \boldsymbol{n[t']} \ \ \ \forall \boldsymbol{t'} \in \boldsymbol{\eta_t}$$
Where, 

  * $[t']$ indicates the elements at pixel location $\boldsymbol{t'}$.
  
  * $\boldsymbol{k_t}$ is the spatially varying PSF at pixel location $\boldsymbol{t}$.

--
  
* We need to estimate $\boldsymbol{k_t}$ for each pixel location $\boldsymbol{t}$ $\implies$ more ill-posed !

--

* We will use parametric models with a small number of parameters.

---

# Proposed Parametric Models for Blur Kernel

* In the case of blurring due to defocus, shape of the blur kernel is **circular** and controls the level of blur.

--

* **Uniform distribution** across a circular are defined by the radius of the circle, denoted by $r$.

$$k(x,y) = \frac{1}{\pi r^2} \times \text{I}_{\{x^2 + y^2 \ \leq \ r^2\}}$$
--

* **Gaussian distribution** across a circular area defined by the radius of the circle, denoted as $r$, and the scale parameter, represented as $h$.

$$k(x,y) = \frac{C_{h,r}}{2\pi h^2} e^{-\frac{x^2 + y^2}{2h^2}} \times \text{I}_{\{x^2 + y^2 \ \leq \ r^2\}}$$
--

* **Cauchy distribution** across a circular area defined by the radius of the circle, denoted as $r$, and the scale parameter $h$.

  $$k(x,y) = \frac{C_{h,r}}{2\pi}\frac{h}{(x^2 + y^2 + h^2)^{3/2}}\times \text{I}_{\{x^2 + y^2 \ \leq \ r^2\}}$$


---

# Prior on Natural Images

* By **natural**, we refer to typical scenes captured in amateur digital photography, excluding specialized contexts like astronomy or satellite imaging.

--

* The prior family used is motivated from the observation that the distribution of image gradients have a **sharp peak near zero** and and relatively **heavier tails** than the Gaussian distribution and Laplace distribution.

```{r ,warning=FALSE,echo=FALSE,out.width='75%',out.height='30%',fig.align='center',echo=FALSE,fig.cap="Figure: Eight sharp images and their density plot of horizontal gradients"}

knitr::include_graphics("pimg/prior1.png")
```

---

# Prior on Natural Images

* A useful parametric family to model this is the so called **Hyper-Laplacian Distribution** given by 

$$f_{\alpha}(z) = \frac{\alpha}{2\Gamma(\frac{1}{\alpha})}\text{exp}{(-|z|^{\alpha})}, z \in \mathbb{R} \ \ \text{and} \ \ \alpha > 0$$

* Levin et al. used $\alpha = 0.8$ and Zhu et al. used $\alpha = 2$  with IID assumption of image gradients.

--

* Nandy (2021) showed that assumption of independent gradients is incorrect and suggested *simple AR process* to model it.

--

* To use these priors we express the blur model in-terms of image gradients in frequency domain.

$$\boldsymbol{\delta_h}\otimes\boldsymbol{b} = \boldsymbol{\delta_h} \otimes(\boldsymbol{k} \ \otimes \ \boldsymbol{l}) \ + \ (\boldsymbol{\delta_h}\otimes \boldsymbol{\epsilon}) =  k \otimes(\boldsymbol{\delta_h} \ \otimes \ \boldsymbol{l}) \ + \ (\boldsymbol{\delta_h}\otimes \boldsymbol{\epsilon})$$
$$\boldsymbol{\delta_v}\otimes\boldsymbol{b} = \boldsymbol{\delta_v} \otimes(\boldsymbol{k} \ \otimes \ \boldsymbol{l}) \ + \ (\boldsymbol{\delta_v}\otimes \boldsymbol{\epsilon}) =  k \otimes(\boldsymbol{\delta_v} \ \otimes \ \boldsymbol{l}) \ + \ (\boldsymbol{\delta_v}\otimes \boldsymbol{\epsilon})$$
--

* We will use a generic form of these equations, given by

$$\boldsymbol{y} = \boldsymbol{k} \ \otimes \ \boldsymbol{x} \ + \ \boldsymbol{n}$$

---

# Prior on Natural Images(Contd.)

* Using Convolution Theorem for *Discrete Fourier Transform* we have

$$\boldsymbol{Y} = \boldsymbol{K} \odot \boldsymbol{X} + \boldsymbol{N}$$
$\ \ \  \ \ \ \ \ \text{}$ Where, $\boldsymbol{Y,K,X}$ and $\boldsymbol{N}$ are the *Discrete Fourier Transform*'s of $\boldsymbol{y,k,x}$ and $\boldsymbol{n}$ respectively.

--

* Then, $\forall \ \boldsymbol{\omega} = (\omega_1,\omega_2)$ we have $\boldsymbol{Y_{\omega} = K_{\omega}X_{\omega} + N_{\omega}}$

--

* Nandy (2021) defined the prior on DFT coefficients as -

  * $X_{\omega}$'s are independently distributed and follow the complex normal distribution $\mathcal{CN}(0,\sigma^2 g_{\omega})$ exactly or asymptotically, depending on whether $\alpha = 2$ or not.
  
  * Simple AR process is used to model the dependence structure of latent image gradients, i.e. $\rho(\boldsymbol{x_{ij},x_{kl}}) = {\rho_1}^{|i-k|}{\rho_2}^{|j-l|}$. 
  
  * Under these assumptions $g_{\omega}$ can be calculated explicitly.

--
  
* Note that the above is for uniform blur model.  

---

# Maximum Likelihood Estimation of Blur Kernel Parameters

* To estimate blur kernel parameters $\theta = (r,h)$ or $r$, we will use maximum likelihood procedure.

--

* If we assume that $N_{\omega} \sim \mathcal{CN}(0,\eta^2 h_{\omega})$ for all $\omega$. Then $|\boldsymbol{Y_{\omega}}|^2 \sim \text{Exp}(\lambda_\omega = \frac{1}{\sigma^2|K_\omega|^2 g_{\omega} + \eta^2 h_{\omega}}) \ \ \ \forall \omega$

--

* The joint pdf of $|\boldsymbol{Y_{\omega}}|^2$'s is given by

$$f_{\theta}(|Y_{\omega}|^2,\forall \omega) = \prod_{\omega} f_{\theta,\omega}(|Y_{\omega}|^2)$$
$\ \ \  \ \ \ \ \ \text{}$  Where, $f_{\theta,\omega}(.)$ denotes the pdf of $\text{Exp}(\lambda_\omega = \frac{1}{\sigma^2|K_\omega|^2 g_{\omega} + \eta^2 h_{\omega}})$.

--

* Assuming independence of vertical and horizontal gradients, the joint likelihood is given by

$$L(\boldsymbol{\theta}) = L_h(\boldsymbol{\theta})\times L_v(\boldsymbol{\theta}) = f_{\theta}(|Y_{h,\omega}|^2,\forall \omega) \times f_{\theta}(|Y_{v,\omega'}|^2,\forall \omega')$$

--

* Our objective is to find

$$\hat{\theta} = \underset{\boldsymbol{\theta}}{\text{argmax}} \ \log L(\boldsymbol{\boldsymbol{\theta}}) = \underset{\theta}{\text{argmax}} \ \ \{\log L_h(\boldsymbol{\theta}) + \log L_v(\boldsymbol{\theta})\}$$

---

# Challenges in ML Estimation


* Parameter $\boldsymbol{\theta}$ is involved in the expression $\lambda_{\omega}$ through $|\boldsymbol{K_{\omega}}|^2$, which is a complicated function of $\boldsymbol{\theta}$.

--

* Thus, before we start using any optimization technique, we empirically investigate the behavior of $L(\boldsymbol{\theta})$ as a function of $\boldsymbol{\theta}$.

--

* Simulated experiments using disc kernel are conducted for this purpose.

* We consider a sequence of values for $r \in [1,4]$ with $\Delta{r} = 0.05$, and for $\sigma \in [0.01,0.4]$ with $\Delta{\sigma} = 0.01$, with $\eta = 0.001$ constant.

--

* We use a numerical optimization approach, explicitly calculating $|\boldsymbol{K_{\omega}}|^2$ as a function of $\boldsymbol{\theta}$.


---

## Experiment - 1

```{r ,warning=FALSE,echo=FALSE,out.width='70%',fig.align='center',echo=FALSE,fig.cap="Figure: (a) 101 x 101 Sharp Image, (b) Blurred Image Using disc kernel with r = 3, (c) Disc Kernel with r = 3, (d) Levelplot of log likelihood as a function of sigma and r"}

knitr::include_graphics("pimg/exp1.png")
```


---

## Experiment - 2

```{r ,warning=FALSE,echo=FALSE,out.width='72%',fig.align='center',echo=FALSE,fig.cap="Figure: (a) 101 x 101 Sharp Image, (b) Blurred Image Using disc kernel with r = 2.5, (c) Disc Kernel with r = 2.5, (d) Levelplot of log likelihood as a function of sigma and r"}

knitr::include_graphics("pimg/exp2.png")
```

---

# Challenges in ML Estimation

* Global maxima don't always correspond to the actual parameters of the blur kernel.

--

* Poor estimation of blur kernel can lead to artifacts.

```{r ,warning=FALSE,echo=FALSE,out.width='65%',fig.align='center',echo=FALSE,fig.cap="Figure: Effect of poor estimation of radius r in disc kernel (Using Richardson Lucy Algorithm)"}

knitr::include_graphics("pimg/deconv_prob.png")
```

---

# Challenges in ML Estimation

* The choice of the prior parameter $\sigma$ is playing an important role.

--

* One reasonable option is to fix a value of $\sigma$.

--

* Simulation study can be used to find a reasonable value of $\sigma$.

--

* We consider five sharp images and true parameter values $r_{true} = 1, 3, 5$.
  
* For each value of $r_{true}$, patches of fixed size are randomly selected, and defocus blur is simulated.

* $\hat{r}$ is determined as a function of $\sigma$ in each case and plotted.

--

* From that $\sigma = 0.2$ seems to be a reasonable choice.

---

## Choice of $\sigma$ : $\small{51 \times 51}$ Patch

```{r ,warning=FALSE,echo=FALSE,out.width='43%',fig.align='center',echo=FALSE}

knitr::include_graphics("pimg/p51.png")
```

---

## Choice of $\sigma$ : $\small{101 \times 101}$ Patch

```{r ,warning=FALSE,echo=FALSE,out.width='43%',fig.align='center',echo=FALSE}

knitr::include_graphics("pimg/p101.png")
```

---

## Choice of $\sigma$ : $\small{201 \times 201}$ Patch

```{r ,warning=FALSE,echo=FALSE,out.width='42%',fig.align='center',echo=FALSE}

knitr::include_graphics("pimg/p201.png")
```

---

# An Application using Local Patches

```{r ,warning=FALSE,echo=FALSE,out.width='60%',fig.align='center',echo=FALSE,fig.cap="Figure: Application on real life image"}

knitr::include_graphics("pimg/ourap.png")
```

---

# Towards Image Segementation

* Instead of manually selecting patches, we require a more general method.

--

* Selecting overlapping local patches for each pixel can be useful.

--

* But it may yield poor results in certain situations.

--

* We will use segments obtained by segmentation algorithm.

---

# Segment Anything

```{r,warning=FALSE,echo=FALSE,out.width='100%',out.height='100%'}
knitr::include_url("https://segment-anything.com/",height = "500px")
```

---

# Segment Anything

* It has both manual and automatic method to perform segmentation.

* It can take **prompts** such as - box, points, texts as input to perform segmentation.

--

```{r ,warning=FALSE,echo=FALSE,out.width='80%',fig.align='center',echo=FALSE,fig.cap="Figure: Automatic and Manual Segmentation by SAM"}

knitr::include_graphics("pimg/couple.png")
```


---

# Future Work

* Perform more simulations involving other types of blur kernels.

--

* Conduct empirical analysis to understand which blur kernel model is applicable.

--

* Evaluate how our method performs for the image segments identified by the segmentation algorithm.

--

* Use our method to find depth map given a single image.

--

* Attempt to use this method for tasks like post-capture image refocusing.

---

# References

* Brian A. Barsky, Daniel R. Horn, and Klein. “Camera Models and Optical Systems Used in Computer Graphics: Part I, Object-Based Techniques”. In: Lecture Notes in Computer Science. Springer Berlin Heidelberg, 2003. url: http://dx.doi.org/10.1007/3-540-44842X_26.

* P. Grossmann. “Depth from focus”. In: Pattern Recognition Letters (1987). issn: 0167-8655.url: http://dx.doi.org/10.1016/0167-8655(87)90026-2.

* Alexander Kirillov et al. Segment Anything. 2023. url: https://arxiv.org/abs/2304.02643.

* Anat Levin et al. “Image and depth from a conventional camera with a coded aperture”. In: ACM transactions on graphics (TOG) 26.3 (2007), 70–es. url: http://dx.doi.org/10.1145/1276377.1276464

*  Kaustav Nandy. “Locally Dependent Natural Image Priors for Non-blind and Blind Image Deconvolution”. PhD thesis. Indian Statistical Institute, 2021. url: 
https://digitalcommons.isical.ac.in/doctoral-theses/7/

* Deepayan Sarkar and Kaustav Nandy. rip: Image Processing in R. New Delhi, India, 2021. url: https://github.com/deepayan/rip.

* Xiang Zhu et al. “Estimating Spatially Varying Defocus Blur From A Single Image”. In: (2013). issn: 1941-0042. url: http://dx.doi.org/10.1109/TIP.2013.2279316.

---

# Appendix

.panelset[
.panel[.panel-name[Thin Lens Model]

* From a single point source, light rays emit in different directions and fall on the lens of camera.

* The lens bends light to form a circle on the camera sensor. It's called the *Blur Circle* or *Circle of Confusion*.

* There is a relation between circle of confusion and depth of object in an image.

$$c_{diam} = a_{diam}f \left|\frac{d - d_{focus}}{d(d_{focus} - f)}\right| \approx a_{diam}f \left|\frac{1}{d_{focus}} - \frac{1}{d}\right|$$

* In a given camera settings, $c_{diam} \propto \left|\frac{1}{d_{focus}} - \frac{1}{d}\right|$ 

* For different values of $d$, we can have $\left|\frac{1}{d_{focus}} - \frac{1}{d}\right|$ same $\implies$ ill possed problem !

]

.panel[.panel-name[Objects Closer]

* For objects closer to the camera than the plane of focus

```{r ,warning=FALSE,echo=FALSE,out.width='80%',fig.align='center',echo=FALSE}

knitr::include_graphics("pimg/d2.png")
```

]

.panel[.panel-name[Objects Farther]

* For objects farther from the camera than the plane of focus

```{r ,warning=FALSE,echo=FALSE,out.width='75%',fig.align='center',echo=FALSE}

knitr::include_graphics("pimg/d1.png")
```

]
]

---

class: center, middle
background-size: cover

# Thank You

All codes and diagrams are available at [ShrayanRoy/DepthR](https://github.com/ShrayanRoy/DepthR)
