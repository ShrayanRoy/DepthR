<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>A Scalable Version of MADD</title>
    <meta charset="utf-8" />
    <meta name="author" content="Adrija Saha, Roll No: MD2203" />
    <script src="libs/header-attrs-2.25/header-attrs.js"></script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="design.css" type="text/css" />
    <link rel="stylesheet" href="fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# A Scalable Version of MADD
]
.subtitle[
## for Classification Problems
]
.author[
### Adrija Saha, Roll No: MD2203
]
.institute[
### Supervisor: Dr. Soham Sarkar (SMU, ISI Delhi)
]
.date[
### 23/05/2024
]

---


class: center, middle
background-image: url("BACK.JPEG")
background-size: cover

&lt;style type="text/css"&gt;
.reduced_opacity {
  opacity: 0.4;
}
.red { color: rgb(200,0,0); }
.green { color: green; }
.blue { color: blue; }
.scroll-1000 {
  max-height: 400px;
  max-width: 1000px;
  overflow-y: auto;
  background-color: inherit;
}
&lt;/style&gt;



&lt;style type="text/css"&gt;
.remark-slide-number {
  display: none;
}
&lt;/style&gt;

&lt;style type="text/css"&gt;
.small-text {
  font-size: 0.8em;
}
&lt;/style&gt;

# Last time...

---
# Introduction

- We were interested in **Classification** problems.

- A popular choice is to consider **K-Nearest Neighbor Classifier** based on the Euclidean distance.

- But in High Dimensional problems, KNN based on the Euclidean distance performs poorly.

--

- If location difference is dominated by scale difference, .red[NN classifier assigns all observations to the population with smaller dispersion!]

---

# Mean Absolute Difference of Distances

* MADD (Sarkar and Ghosh 2019) is a semi-metric based on available data cloud, defined as: 

`$$\rho(\mathbf{x},\mathbf{y})= \frac{1}{n - 2} \sum_{\mathbf{z} \in \mathcal{X} \setminus \{\mathbf{x},\mathbf{y}\}} \big|\|\mathbf{x}- \mathbf{z}\| - \|\mathbf{y}-\mathbf{z}\|\big|$$`
* Roy et. al. 2022 used MADD for high dimension, low sample size classification problems.


* Computation of MADD between two points requires `\(\mathcal{O}(nd)\)` operations.


* Complexity becomes `\(\mathcal{O}(n^2d)\)` for classifying a single observation `\(\implies\)` Quadratic in `\(n\)`.

---

# High Dimensional Behavior of MADD

* If `\(\mathbf{X} \sim F_1, \mathbf{Y} \sim F_2\)` are two independent observations, then under certain conditions

`$$d^{-1/2}||\mathbf{X}-\mathbf{Y}|| \xrightarrow{P} \sqrt{\nu_{12}^2+\sigma_1^2+\sigma_2^2} \ \ \text{as} \ \ d \rightarrow \infty$$`

* Let us look at the expression,

`$$\rho_0(\mathbf{X},\mathbf{Y}) = d^{-1/2}\left[\frac{1}{n - 2} \sum_{\mathbf{Z} \in \mathcal{X} \setminus \{\mathbf{X},\mathbf{Y}\}} \big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|\right]$$`

`$$= \frac{1}{n - 2} \left\{\sum_{\mathbf{Z} \in \mathcal{X}_1 \setminus \{\mathbf{X}\}} \underbrace{d^{-1/2}\big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|}+
\sum_{\mathbf{Z} \in \mathcal{X}_2 \setminus \{\mathbf{Y}\}} d^{-1/2}\big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|\right\}$$`

---

# High Dimensional Behavior of MADD

* If `\(\mathbf{X} \sim F_1, \mathbf{Y} \sim F_2\)` are two independent observations, then under certain conditions

`$$d^{-1/2}||\mathbf{X}-\mathbf{Y}|| \xrightarrow{P} \sqrt{\nu_{12}^2+\sigma_1^2+\sigma_2^2} \ \ \text{as} \ \ d \rightarrow \infty$$`
* Let us look at the expression,

`$$\rho_0(\mathbf{X},\mathbf{Y}) = d^{-1/2}\left[\frac{1}{n - 2} \sum_{\mathbf{Z} \in \mathcal{X} \setminus \{\mathbf{X},\mathbf{Y}\}} \big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|\right]$$`

`$$= \frac{1}{n - 2} \left\{\sum_{\mathbf{Z} \in \mathcal{X}_1 \setminus \{\mathbf{X}\}} d^{-1/2}\big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|+
\sum_{\mathbf{Z} \in \mathcal{X}_2 \setminus \{\mathbf{Y}\}} \underbrace{d^{-1/2}\big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|}\right\}$$`


---

# Modified Version of MADD

The modified version of MADD, will be of the form:

`$$\rho_{Mod}(\mathbf{x},\mathbf{y}) = \frac{1}{|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|} \sum_{\mathbf{z} \in \mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}} \big| \|\mathbf{x}- \mathbf{z}\| - \|\mathbf{y}- \mathbf{z}\|\big|$$`
Where,

   * `\(|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|\)` denotes the cardinality of `\(\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}\)`.
  
   * `\(\mathcal{X}^{\ast} \subset \mathcal{X}\)`.
  
   * `\(\mathcal{X}^{\ast} \cap \mathcal{X}_j \neq \phi, \quad \text{for } j= 1,2.\)`

---

# Strategy 01: SRSWOR

&lt;img src="SRSWOR.JPEG" width="100%" style="display: block; margin: auto;" /&gt;

* .green[Straightforward choice]


* .red[Does not consider diversity in the data structure] `\(\implies\)` .red[May lack in representing the whole sample.]

---

# Strategy 02: Determinantal Point Process

.panelset[
.panel[.panel-name[DPP]
* DPP creates a repulsion between points leading to the selection of a diverse set.

* A point process `\(\mathcal{P}\)` on a discrete set `\(\mathcal{Y}\)` `\(= \{1, . . . , N\}\)` is a probability measure on `\(2^{\mathcal{Y}}\)`, the set of all subsets of `\(\mathcal{Y}\)`. 

* It is said to be a DPP, if for a random set **Y** drawn as per `\(\mathcal{P}\)`, `\(\mathcal{P}(A \subseteq \boldsymbol{Y}) = det(K_A)\)` for every subset `\(A \subseteq \mathcal{Y}\)`.

`$$\mathcal{P}(i \in \boldsymbol{Y}) = K_{ii}, \quad \mathcal{P}(i \in \boldsymbol{Y}, j \in \boldsymbol{Y}) = K_{ii}K_{jj}-K_{ij}^2, \quad \text{for all }i,j \in \mathcal{Y}$$`
* The general definition of DPP gives the probability of **inclusion** of a subset.
]
.panel[.panel-name[L-Ensemble]

* Defines a DPP through a positive semi-definite matrix `\(L\)` indexed by elements of `\(\mathcal{Y}\)`.
`$$\mathcal{P}_L(\boldsymbol{Y}= A) = \text{det}(L_A)/{\text{det}(L + I)}$$`

* Directly represent the probabilities of observing each subset of `\(\mathcal{Y}\)`.

]
.panel[.panel-name[k-DPP]

* A k-DPP selects exactly k points according to a DPP.

* DPP conditioned on the cardinality of the selected subset.
]
]

---
# Strategy 02: Determinantal Point Process

* .green[Considers the diversity in the data points.] 

* .red[Depends on the choice of Kernel Matrix.]

---

# DPP-1 

* Let us take `\(L= XX'\)`, where `\(X_{n \times p}\)` is the data matrix.

* Denote the rows of `\(X\)` by `\(\{\mathbf{X_i}\}_{i=1}^n\)`, `\(\mathcal{P}_L(\mathbf{Y}) \propto det(L_\mathbf{Y}) = Vol^2(\{\mathbf{X_i}\}_{i \in \mathbf{Y}})\)`.

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="dpp_parallel.jpeg" alt="Figure: Geometrical View of DPPs (Source: Kulesza and Taskar 2012)" width="50%" /&gt;
&lt;p class="caption"&gt;Figure: Geometrical View of DPPs (Source: Kulesza and Taskar 2012)&lt;/p&gt;
&lt;/div&gt;

---

# DPP-2

* In a `\(2\)`-DPP problem, where `\(L=((L_{i,j}))\)`, with `\(L_{i,j}= e^{-\frac{||\mathbf{x_i}-\mathbf{x_j}||^2}{d}}\)`. 

* For the set `\(A=\{\mathbf{x_i},\mathbf{x_j}\}\)`, `$$\mathcal{P}_L(A) \propto det(L_A) = 1 - e^{-\frac{2||\mathbf{x_i}-\mathbf{x_j}||^2}{d}}$$`

* As the distance between `\(\mathbf{x_i}\)` and `\(\mathbf{x_j}\)` increases, the probability of their selection also increases.


* Thus we consider *Radial-Basis Function Kernel* between the data points present in each population.


* For two data points `\(\mathbf{x_i}\)` and `\(\mathbf{x_j}\)`, it is defined as `$$L(\mathbf{x_i},\mathbf{x_j})=e^{-\frac{||\mathbf{x_i}-\mathbf{x_j}||^2}{d}}$$`

* Being a kernel it will result in a positive semi-definite symmetric matrix (Lanckriet et al. 2002).

---

# Simulation Studies 

.green[Experiment Specifications:]

&gt;- `\(5\)`-nearest neighbor classifier

&gt;- Test set size: `\(500\)` ( `\(250\)` observations from each class)

&gt;- Sample sizes: `\(n_1 = n_2 = n = 50, 100\)`

&gt;- Dimensionality: `\(d = 20, 50, 100\)`

&gt;- No. of points Selected from each population: `\(k = 2, 4, 8, 16\)`

&gt;- `\(100\)` replications considered

---

### Simulation 01: A Pure Location Problem

* Population 1 `\(\equiv N_d(\mathbf{0},I_{d})\)` &amp; Population 2 `\(\equiv N_d(0.5\mathbf{1_{d}},I_{d})\)`


















.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-12-1.png" alt="Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-13-1.png" alt="Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-14-1.png" alt="Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
]

---

### Simulation 02: A Pure Scale Problem 

* Population 1 `\(\equiv N_d(\mathbf{0},I_{d})\)` &amp; Population 2 `\(\equiv N_d(\mathbf{0},2I_{d})\)`



















.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-20-1.png" alt="Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-21-1.png" alt="Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-22-1.png" alt="Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
]

---
###  Simulation 03: Location Problem with Autocorrelated Features

* .small-text[Population 1] `\(\small{\equiv Y_j= Y_{j-1}+\epsilon_j, \epsilon_j \sim^{ind} N(0,1)}\)` .small-text[&amp; Population 2] `\(\small{\equiv Y_j-0.5 = 0.5(Y_{j-1}-0.5)+\epsilon^*_j, \epsilon^*_j \sim^{ind} N(0,1)}\)`


.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-23-1.png" alt="Figure: Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-24-1.png" alt="Figure: Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-25-1.png" alt="Figure: Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---

## Comparison of Computing times
















.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-31-1.png" alt="Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-32-1.png" alt="Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-33-1.png" alt="Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---

class: center, middle
background-image: url("BACK.JPEG")
background-size: cover

# Extension to Multi-Class Classification Problem

---

# Scalable Version of MADD

`$$\rho_{Mod}(\mathbf{x},\mathbf{y}) = \frac{1}{|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|} \sum_{\mathbf{z} \in \mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}} \big| \|\mathbf{x}- \mathbf{z}\| - \|\mathbf{y}- \mathbf{z}\|\big|$$`
Where,

   * `\(|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|\)` denotes the cardinality of `\(\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}\)`.
  
   * `\(\mathcal{X}^{\ast} \subset \mathcal{X}\)`.
  
   * `\(\mathcal{X}^{\ast} \cap \mathcal{X}_j \neq \phi, \quad \text{for } j= 1,2,...,J.\)`

---

### Misclassification Rate for a Three-Class Pure Location Problem

* Population 1 `\(\equiv N_d(-0.5\mathbf{1_{d}},I_{d})\)` &amp; Population 2 `\(\equiv N_d(\mathbf{0},I_{d})\)` &amp; Population 3 `\(\equiv N_d(0.5\mathbf{1_{d}},I_{d})\)`


.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-34-1.png" alt="Figure: Misclassification rates (in %) in a Pure Location Problem for a 3-class classification problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Location Problem for a 3-class classification problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-35-1.png" alt="Figure: Misclassification rates (in %) in a Pure Location Problem for a 3-class classification problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Location Problem for a 3-class classification problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---
# Comparison of Computing Times


.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-36-1.png" alt="Figure: Computing times taken (in secs.) in a 3-class classification problem with only location difference. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Computing times taken (in secs.) in a 3-class classification problem with only location difference. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-37-1.png" alt="Figure: Computing times taken (in secs.) in a 3-class classification problem with only location difference. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Computing times taken (in secs.) in a 3-class classification problem with only location difference. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---

class: center, middle
background-image: url("BACK.JPEG")
background-size: cover

# Scalable Version of g-MADD

---


# Generalized MADD: An Overview

- Usual MADD is only confined to the cases where populations either differ in their location or in their total variance.

- For two vectors `\(\mathbf{x}\)` and `\(\mathbf{y}\)`, define,

`\begin{equation}
\label{1.1.4}
\rho_{h,\psi}(\mathbf{x}, \mathbf{y}) = \frac{1}{n - 2} \sum_{\mathbf{z} \in \mathcal{X} \setminus \{\mathbf{x},\mathbf{y}\}} \big| \phi_{h,\psi}(\mathbf{x}, \mathbf{z}) - \phi_{h,\psi}(\mathbf{y}, \mathbf{z}) \big|
\end{equation}`
Where,
  * `\(h : \mathbb{R}^+ \rightarrow \mathbb{R}^+\)` and `\(\psi : \mathbb{R}^+ \rightarrow \mathbb{R}^+\)` continuous, monotonically increasing, `\(h(0) = \psi(0) = 0\)` such that `\(\phi_{h,\psi}(\mathbf{x}, \mathbf{y}) = h\left( \frac{1}{d} \sum_{q=1}^{d} \psi \left( |x^{(q)} - y^{(q)}| \right) \right).\)` 

- Here, we will take `\(h(t)=t\)`, `\(\psi(t)=1-e^{-t}\)`.

---
# Computational Challenges with g-MADD
- Again, the computational complexity becomes `\(O(n^2d)\)` for classifying single observation.

--

- We will propose a scalable version of g-MADD of the form: `$$\rho^{Mod}_{h,\psi}(\mathbf{x},\mathbf{y}) = \frac{1}{|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|} \sum_{\mathbf{z} \in \mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}} \big| \phi_{h,\psi}(\mathbf{x}, \mathbf{z}) - \phi_{h,\psi}(\mathbf{y}, \mathbf{z})\big|$$`
  where `\(\mathcal{X}^{\ast} \text{ is a subset of } \mathcal{X}\)` such that `\(\mathcal{X}^{\ast} \cap \mathcal{X}_j \neq \phi, \quad \text{for } j=1,2.\)` 

---
# Choice of L-ensemble Matrix

### Generalization of DPP-1

- Euclidean distance is directly related to the volume of the parallelepiped.
- No such direct connection for a general distance.

--

### Generalization of DPP-2

- Recall DPP-2: Used `\(L(\mathbf{x_i}, \mathbf{x_j}) = e^{-\frac{||\mathbf{x_i} - \mathbf{x_j}||^2}{d}}\)`.

- For g-MADD, propose: `\(L(\mathbf{x_i}, \mathbf{x_j}) = e^{-\frac{1}{d} \sum_{q=1}^{d} \psi( |x_i^{(q)} - x_j^{(q)}|)}\)`.

- Replace Euclidean distance with a general distance function. 

---
### Simulation: Features are Independent Normal vs Features are Independent t

* .small-text[Both the population have mean] `\(\small{\mathbf{0}}\)` .small-text[and dispersion] `\(\small{3I_d}\)`.

.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-38-1.png" alt="Figure: Misclassification rates (in %) when the features are independent normal vs when the features are independent t. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) when the features are independent normal vs when the features are independent t. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-39-1.png" alt="Figure: Misclassification rates (in %) when the features are independent normal vs when the features are independent t. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) when the features are independent normal vs when the features are independent t. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-40-1.png" alt="Figure: Misclassification rates (in %) when the features are independent normal vs when the features are independent t. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) when the features are independent normal vs when the features are independent t. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---

## Comparison of Computing Times


.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-41-1.png" alt="Figure: Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-42-1.png" alt="Figure: Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-43-1.png" alt="Figure: Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---
## Comparison of Scalable Version of MADD and g-MADD: Misclassification Rate and Computing Time

* Population 1 `\(\equiv N_d(\mathbf{0},I_{d})\)` &amp; Population 2 `\(\equiv N_d(0.5\mathbf{1_{d}},I_{d})\)` ( `\(n=\)` `\(50\)`, `\(d=\)` `\(50\)`)

&lt;img src="pgli1.JPEG" width="100%" height="100%" style="display: block; margin: auto;" /&gt;

&lt;table class="table table-striped table-hover table-condensed table-responsive" style="color: black; width: auto !important; "&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; Misclassification Rate (in %) &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; SE &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; Computing Time (in sec.) &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; SE &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Traditional MADD &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1521 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.9966 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5592 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Traditional g-MADD &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.26 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1305 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.9581 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7411 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
class: center, middle

.pull-left[

# Application on Benchmark Dataset: Fashion MNIST

&gt;- Contains `\(60,000\)` grayscale images of size `\(28 \times 28\)` of the `\(10\)` fashion article classes.

&gt;- Also contains a test set of `\(10,000\)` images.
]

.pull-right[

&lt;img src="fashion-mnist-sprite.JPEG" width="100%" style="display: block; margin: auto;" /&gt;
]

---
## Defining Distance Metric, MADD and g-MADD for Image Data

- Suppose each grayscale image has `\(p \times p\)` pixels, totaling `\(D = p^2\)` pixels.


- The distance between two images `\(I_1\)` and `\(I_2\)` is calculated as:
  `$$d(I_1, I_2) = \sqrt{\sum_{d=1}^D (I_1^d - I_2^d)^2}$$`

- This formula treats pixel values as feature values for each image.

- It is indeed the usual distance after vectorizing the images.
--

- Following the same concept, MADD and g-MADD and their Scalable versions are also consistent with the usual ones.

---
## Two-Class Classification Problem
- Randomly chosen `\(200\)` observations from `Sandal` and `\(200\)` observations from class `Sneaker` as training observations. 
&lt;img src="sandal_vs_sneaker.JPEG" width="100%" style="display: block; margin: auto;" /&gt;

---

## Results

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/Figure-13-1.png" alt="Figure: Misclassification rates (in %) (left) and computing time taken (in sec.) (right) for classification of Sandal and Sneaker using 200 training observations (from each class) from MNIST FASHION dataset."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) (left) and computing time taken (in sec.) (right) for classification of Sandal and Sneaker using 200 training observations (from each class) from MNIST FASHION dataset.&lt;/p&gt;
&lt;/div&gt;

---
## Three-Class Classification Problem
- Randomly chosen `\(200\)` observations to construct the training set. 
&lt;img src="sandal_sneaker_ankle_boot.JPEG" width="100%" style="display: block; margin: auto;" /&gt;

---
## Results

* Here, we have only tabulated the scalable version's performance with `\(k = 16\)`.

* We have not used g-MADD here, since previously, we have not seen any better performance using g-MADD.
&lt;table class="table table-striped table-hover table-condensed table-responsive" style="color: black; width: auto !important; "&gt;
&lt;caption&gt;Table: Misclassification Rate (in %) and Computing Time (in sec.) for classification of Sandal, Sneaker, Ankle boot using 200 training observations (from each class) from MNIST FASHION dataset.&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; Misclassification Rate (in %) &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; Computing Time (in sec.) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; DPP 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1551.882 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; DPP 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12.03 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1554.737 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; SRSWOR &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12.93 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1543.262 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Traditional MADD &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12.40 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18766.605 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Euclidean &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 14.53 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15.577 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

class: center, middle
background-image: url("BACK.JPEG")
background-size: cover

# Concluding Discussions

---
# DPP or SRSWOR?



.pull-left[


&lt;img src="WOR_vs_DPP.jpeg" width="100%" style="display: block; margin: auto;" /&gt;


]
.pull-right[

- One may opt for the scalable version using DPP.

- DPP considers the diversity in the sample.

- DPP showed slight improvement in some cases.

- In real data analysis, DPP performed much better with the same `\(k\)`.

- Computing time for DPP and SRSWOR is very close.

]
---
## Choice of k

- A crucial part of our Method is deciding the number of observations to draw from each population to balance accuracy and computing time.

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/Figure-11-1.png" alt="Figure: Misclassification rates (left) and computing time taken (in sec.) (right) for a pure scale problem with scalable version of MADD computed based on DPP-2. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (left) and computing time taken (in sec.) (right) for a pure scale problem with scalable version of MADD computed based on DPP-2. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;

---
## Comparison with Existing Method for Dealing with Computational Issues 

Pal et al. (2016) inspired from Condensed Nearest Neighbor (Hart 1968) or Reduced Nearest Neighbor (Gates 1972) used the following algorithm.

&lt;img src="CNN_final.JPEG" width="80%" style="display: block; margin: auto;" /&gt;


---

## Comparison with Existing Method for Dealing with Computational Issues 

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pgli2.JPEG" alt="Figure: Misclassification rate (in %) (left) and computing time taken (in sec.) (right) for a pure location problem with scalable version of MADD computed based on DPP-2 and based on CNN with varying threshold." width="90%" /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rate (in %) (left) and computing time taken (in sec.) (right) for a pure location problem with scalable version of MADD computed based on DPP-2 and based on CNN with varying threshold.&lt;/p&gt;
&lt;/div&gt;

---

# References

* Gates, G. (1972). The reduced nearest neighbor rule (corresp.). *IEEE Transactions on Information Theory*, 18(3):431–433.

* Hall, P., Marron, J. S., and Neeman, A. (2005). Geometric representation of high dimension, low sample size data. *Journal of the Royal Statistical Society Series B*, 67(3):427–444.

* Hart, P. (1968). The condensed nearest neighbor rule. *IEEE Transactions on Information Theory*, 14(3):515–516.

* Pal, A. K., Mondal, P. K., and Ghosh, A. K. (2016). High dimensional nearest neighbor classification based on mean absolute differences of inter-point distances. *Pattern Recognition Letters*, 74:1–8.

* Lanckriet, G., Cristianini, N., Bartlett, P., Ghaoui, L., and Jordan, M. (2002). Learning the Kernel Matrix with Semi-Definite Programming. *Journal of Machine Learning Research*, 5:323–330.

* Hall, P., Marron, J. S., and Neeman, A. (2005). Geometric representation of high dimension, low sample size data. *Journal of the Royal Statistical Society Series B*, 67(3):427–444.

---

# References

* Hastie, T., Tibshirani, R., and Friedman, J. H. (2009). *The Elements of Statistical Learning. Springer Series in Statistics.* Springer, New York.

* Kulesza, A. and Taskar, B. (2012). Determinantal Point Processes for Machine Learning. *Foundations and Trends® in Machine Learning,* 5(2-3):123–286

* Sarkar, S. and Ghosh, A. (2019). On Perfect Clustering of High Dimension, Low Sample Size Data. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 42(9):2257–2272

* Roy, S., Sarkar, S., Dutta, S., and Ghosh, A. K. (2022). On Generalizations of Some Distance Based Classifiers for HDLSS Data. *Journal of Machine Learning Research*, 23(14):1–41.

---

class: center, middle
background-image: url("BACK.JPEG")
background-size: cover

# Thank You

All codes are available in [Adrija211200/ScalableMADDR](https://github.com/Adrija211200/ScalableMADDR)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
