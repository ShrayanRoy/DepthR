<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Depth Estimation Using</title>
    <meta charset="utf-8" />
    <meta name="author" content="Shrayan Roy, Roll No: MD2220" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="isi.css" type="text/css" />
    <link rel="stylesheet" href="fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Depth Estimation Using
]
.subtitle[
## Segmentation in Natural Images
]
.author[
### Shrayan Roy, Roll No: MD2220
]
.institute[
### Supervisor: Dr. Deepayan Sarkar
]
.date[
### Indian Statistical Institute, Kolkata
]

---






&lt;style type="text/css"&gt;
.remark-slide-number {
  display: none;
}
&lt;/style&gt;

# Introduction

* Humans possess a natural ability to perceive *3D structure* from 2D images.

--

* Primarily relying on *visual cues* such as perspective, relative object sizes.

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="dof.jpg" alt="Figure: 3D perspective from 2D image" width="65%" /&gt;
&lt;p class="caption"&gt;Figure: 3D perspective from 2D image&lt;/p&gt;
&lt;/div&gt;

---

# Depth: the third dimension

* Traditional photographs are two dimensional projections of a three dimensional scene.

--

* The third dimension is *depth*, which represents the distance between camera and objects in the image.

--

* It provides **spatial information** about the scene, which can be used for applications such as post-capture image refocusing, automatic scene segmentation and object detection.

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="persproj.png" alt="Figure: 2D Projection of 3D scene in standard cameras" width="65%" /&gt;
&lt;p class="caption"&gt;Figure: 2D Projection of 3D scene in standard cameras&lt;/p&gt;
&lt;/div&gt;

---

# Existing Methods to Estimate Depth

* Most modifications to estimate depth involves **multiple images** of the same scene or some **hardware based solution** such as light emitters, coded aperture.

--

* These methods are **not** applicable in practice, as it requires modifying the camera system before capturing the image, which may not always be feasible.

--

* Ideally, we aim to estimate the depth map given a single image of the scene.

--

* Depth estimation from a single image is a more challenging problem because we have only a **single observation** for each pixel of the scene.


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="dest.png" alt="Figure: Methods to estimate depth" width="90%" /&gt;
&lt;p class="caption"&gt;Figure: Methods to estimate depth&lt;/p&gt;
&lt;/div&gt;

---

# Depth from Defocus

* Depth estimation from defocus blur exploits the phenomenon where objects appear more blurred depending on their distance from the camera lens.

--

* Therefore, measuring the amount of blur at a point of an image can provide a way to recover the depth to the respective point in the 3D world.

--


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="depfoc.png" alt="Figure: Bluriness depends on distance of objects from camera" width="45%" /&gt;
&lt;p class="caption"&gt;Figure: Bluriness depends on distance of objects from camera&lt;/p&gt;
&lt;/div&gt;

---

# Levin et al. (2007)

* Levin et al. utilized this concept to estimate the level of blur per pixel in a given input image.

* They used *sparse gradient prior* on natural images.

* However, this method requires a modified camera with a special coded aperture, rendering it impractical for use with a given input image.


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="levin.png" alt="Figure: Levin et al. 2007 - Image and Depth from a Conventional Camera with a Coded Aperture" width="75%" /&gt;
&lt;p class="caption"&gt;Figure: Levin et al. 2007 - Image and Depth from a Conventional Camera with a Coded Aperture&lt;/p&gt;
&lt;/div&gt;

---

# Zhu et al. (2013)

* Zhu et al. employed the concept of depth from defocus blur to estimate the level of blur per pixel in a given input image.

* They utilized Gabor filters for local frequency component analysis and used *simple gradient prior* on natural images.

* However, this method does not require a special coded aperture.

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="zhu.png" alt="Figure: Zhu et al. 2013 - Estimating Spatially Varying Defocus Blur from A Single Image" width="85%" /&gt;
&lt;p class="caption"&gt;Figure: Zhu et al. 2013 - Estimating Spatially Varying Defocus Blur from A Single Image&lt;/p&gt;
&lt;/div&gt;

---

# Point Spread Function

* When light rays spread from a single point source and hit the camera lens, they should ideally get refracted and converge on the pixel corresponding to the original scene.

--

* However, if the source is out of focus, the refracted rays spread out over neighboring pixels as well.

--

* This spreading pattern, determined by the object’s distance from the lens or camera movement, is called the Point Spread Function (PSF) or Blur Kernel.

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="bkernel.png" alt="Figure: Point Spread Function" width="75%" /&gt;
&lt;p class="caption"&gt;Figure: Point Spread Function&lt;/p&gt;
&lt;/div&gt;

---

# Model for Blurred Image

* The blurred image can be viewed as **convolution** of original sharp image and Point Spread Function. 

* The observed blurred image `\(\boldsymbol{b}\)` of dimension `\(M \times N\)` can be modeled as - 

  `$$\boldsymbol{b} = \boldsymbol{k} \ \otimes \ \boldsymbol{l} \ + \ \boldsymbol{\epsilon}$$`
Where,

  * `\(\boldsymbol{k}\)` is an `\(m \times n\)` PSF / blur kernel.
  
  * `\(\boldsymbol{l}\)` is the `\((M + m) \times (N + n)\)` *true latent image* which we want to estimate.
  
  * `\(\boldsymbol{\epsilon}\)` is an `\(M \times N\)` matrix of noise.
  
  * `\(\otimes\)` denotes the *valid convolution* operator.
  
* Estimating both `\(\boldsymbol{l}\)` and `\(\boldsymbol{k}\)` using above model is called *blind deconvolution problem*.

--

* Number of observations `\(MN\)` is very large compared number of parameters `\(\implies\)` ill-possed problem 

--

* Can use some prior on `\(\boldsymbol{l}\)` to deal with this ? 

---

# Model for Blurred Image (Contd.)


* The model defined in last slide assumes that PSF is *shift invariant* i.e. same PSF applies to all pixels.

--

* In the context of defocus blur, PSF/ Blur Kernel is *spatially varying*.

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="svarying.png" alt="Figure: Spatially Varying Blur Kernel" width="50%" /&gt;
&lt;p class="caption"&gt;Figure: Spatially Varying Blur Kernel&lt;/p&gt;
&lt;/div&gt;


---

# Model for Blurred Image (Contd.)


* The model defined in last slide assumes that PSF is *shift invariant* i.e. same PSF applies to all pixels. 

* In the context of defocus blur, PSF/ Blur Kernel is *spatially varying*.

* We will assume that `\(\boldsymbol{k_t}\)` is shift invariant in a neighborhood `\({\boldsymbol{\eta_t}}\)` of size `\(p_1(\boldsymbol{t}) \times p_2(\boldsymbol{t})\)` containing `\(\boldsymbol{t}\)`.

--

* Based on this, we redefine the *uniform blur model* as - 

  `$$\boldsymbol{y[t']} = (\boldsymbol{k_t} \ \otimes \ \boldsymbol{x})\boldsymbol{[t']} \ + \ \boldsymbol{n[t']} \ \ \ \forall \boldsymbol{t'} \in \boldsymbol{\eta_t}$$`
Where, 

  * `\([t']\)` indicates the elements at pixel location `\(\boldsymbol{t'}\)`.
  
  * `\(\boldsymbol{k_t}\)` is the spatially varying PSF at pixel location `\(\boldsymbol{t}\)`.

--
  
* We need to estimate `\(\boldsymbol{k_t}\)` for each pixel location `\(\boldsymbol{t}\)` `\(\implies\)` more ill possed !

--

* What if assume some form of blur kernel ? For example- *Bivariate Normal distribution*.

---

# Proposed Parametric Models for Blur Kernel

.panelset[
.panel[.panel-name[Thin Lens Model]

* From a single point source, light rays emit in different directions and fall on the lens of camera.

* The lens bends light to form a circle on the camera sensor. It's called the *Blur Circle* or *Circle of Confusion*.

* There is a relation between circle of confusion and depth of object in an image.

`$$c_{diam} = a_{diam}f \left|\frac{d - d_{focus}}{d(d_{focus} - f)}\right| \approx a_{diam}f \left|\frac{1}{d_{focus}} - \frac{1}{d}\right|$$`

* In a given camera settings, `\(c_{diam} \propto \left|\frac{1}{d_{focus}} - \frac{1}{d}\right|\)` 

* For different values of `\(d\)`, we can have `\(\left|\frac{1}{d_{focus}} - \frac{1}{d}\right|\)` same `\(\implies\)` ill possed problem !

]

.panel[.panel-name[Objects Closer]

* For objects closer to the camera than the plane of focus

&lt;img src="d1.png" width="80%" style="display: block; margin: auto;" /&gt;

]

.panel[.panel-name[Objects Farther]

* For objects farther from the camera than the plane of focus

&lt;img src="d2.png" width="80%" style="display: block; margin: auto;" /&gt;

]

.panel[.panel-name[Choice of Kernels]

* **Uniform distribution** across a circular are defined by the radius of the circle, denoted by `\(r\)`.

`$$k(x,y) = \frac{1}{\pi r^2} \times \text{I}_{\{x^2 + y^2 \ \leq \ r^2\}}$$`

* **Gaussian distribution** across a circular area defined by the radius of the circle, denoted as `\(r\)`, and the scale parameter, represented as `\(h\)`.

`$$k(x,y) = \frac{C_{h,r}}{2\pi h^2} e^{-\frac{x^2 + y^2}{2h^2}} \times \text{I}_{\{x^2 + y^2 \ \leq \ r^2\}}$$`

* **Cauchy distribution** across a circular area defined by the radius of the circle, denoted as `\(r\)`, and the scale parameter `\(h\)`.

  `$$k(x,y) = \frac{C_{h,r}}{2\pi}\frac{h}{(x^2 + y^2 + h^2)^{3/2}}\times \text{I}_{\{x^2 + y^2 \ \leq \ r^2\}}$$`

]
]

---

# Prior on Natural Images

* By *natural*, we refer to typical scenes captured in amateur digital photography, excluding specialized contexts like astronomy or satellite imaging. 

* The prior family used is motivated from the observation that the distribution of image gradients have a **sharp peak near zero** and and relatively **heavier tails** than the Gaussian distribution and Laplace distribution.

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="prior1.png" alt="Figure: Eight sharp images and their density plot of horizontal gradients" width="75%" height="30%" /&gt;
&lt;p class="caption"&gt;Figure: Eight sharp images and their density plot of horizontal gradients&lt;/p&gt;
&lt;/div&gt;

---

# Prior on Natural Images(Contd.)

* A useful parametric family to model this is the so called **Hyper-Laplacian Distribution** given by 

`$$f_{\alpha}(z) = \frac{\alpha}{2\Gamma(\frac{1}{\alpha})}\text{exp}{(-|z|^{\alpha})}, z \in \mathbb{R} \ \ \text{and} \ \ \alpha &gt; 0$$`

* Levin et al. used `\(\alpha = 0.8\)` and Zhu et al. used `\(\alpha = 2\)` in their work to model the marginal distribution of image gradients with IID assumption.

--

* Nandy (2021) showed that assumption of independent gradients is not incorrect and suggested *simple AR process* to model the dependence structure.

--

* To use these priors we need to express the blur model in-terms of image gradients in frequency domain.

`$$\boldsymbol{\delta_h}\otimes\boldsymbol{b} = \boldsymbol{\delta_h} \otimes(\boldsymbol{k} \ \otimes \ \boldsymbol{l}) \ + \ (\boldsymbol{\delta_h}\otimes \boldsymbol{\epsilon}) =  k \otimes(\boldsymbol{\delta_h} \ \otimes \ \boldsymbol{l}) \ + \ (\boldsymbol{\delta_h}\otimes \boldsymbol{\epsilon})$$`
`$$\boldsymbol{\delta_v}\otimes\boldsymbol{b} = \boldsymbol{\delta_v} \otimes(\boldsymbol{k} \ \otimes \ \boldsymbol{l}) \ + \ (\boldsymbol{\delta_v}\otimes \boldsymbol{\epsilon}) =  k \otimes(\boldsymbol{\delta_v} \ \otimes \ \boldsymbol{l}) \ + \ (\boldsymbol{\delta_v}\otimes \boldsymbol{\epsilon})$$`
--

* Combining the last two equations we have

`$$\boldsymbol{y} = \boldsymbol{k} \ \otimes \ \boldsymbol{x} \ + \ \boldsymbol{n}$$`

---

# Prior on Natural Images(Contd.)

* Using Convolution Theorem for *Discrete Fourier Transform* we have

`$$\boldsymbol{Y} = \boldsymbol{K} \odot \boldsymbol{X} + \boldsymbol{N}$$`
`\(\ \ \  \ \ \ \ \ \text{}\)` Where, `\(\boldsymbol{Y,K,X}\)` and `\(\boldsymbol{N}\)` are the *Discrete Fourier Transform*'s of `\(\boldsymbol{y,k,x}\)` and `\(\boldsymbol{n}\)` respectively.

--

* Then `\(\forall \ \boldsymbol{\omega} = (\omega_1,\omega_2)\)` we have

`$$\boldsymbol{Y_{\omega} = K_{\omega}X_{\omega} + N_{\omega}}$$`
--

* Nandy (2021) defined the prior on DFT coefficients as -

  * `\(X_{\omega}\)`'s are independently distributed and follow the complex normal distribution `\(\mathcal{CN}(0,\sigma^2 g_{\omega})\)` exactly or asymptotically, contingent on whether `\(\alpha = 2\)` or not.
  
  * Simple AR process is used to model the dependence structure of latent image gradients, i.e. `\(\rho(\boldsymbol{x_{ij},x_{kl}}) = {\rho_1}^{|i-k|}{\rho_2}^{|j-l|}\)`. Under this assumption `\(g_{\omega}\)` can be calculated explicitly.

--

* For spatially varying case, we simply apply these priors to the local patches of the image.

---

# Maximum Likelihood Estimation

* To estimate blur kernel parameters `\(\theta = (r,\sigma)\)` or `\(r\)`, we will use maximum likelihood procedure based on joint distribution of `\(\boldsymbol{y}\)` or equivalently `\(\boldsymbol{Y_{\omega}}\)`'s.

--

* If we assume that `\(N_{\omega} \sim \mathcal{CN}(0,\eta^2 h_{\omega})\)` for all `\(\omega\)`. Then `\(|\boldsymbol{Y_{\omega}}|^2 \sim \text{Exp}(\lambda_\omega = \frac{1}{\sigma^2|K_\omega|^2 g_{\omega} + \eta^2 h_{\omega}}) \ \ \ \forall \omega\)`

--

* The joint pdf of `\(|\boldsymbol{Y_{\omega}}|^2\)`'s is given by

`$$f_{\theta}(|Y_{\omega}|^2,\forall \omega) = \prod_{\omega} f_{\theta,\omega}(|Y_{\omega}|^2)$$`
`\(\ \ \  \ \ \ \ \ \text{}\)`  Where, `\(f_{\theta,\omega}(.)\)` denotes the pdf of `\(\text{Exp}(\lambda_\omega = \frac{1}{\sigma^2|K_\omega|^2 g_{\omega} + \eta^2 h_{\omega}})\)`.

--

* By assuming independence of vertical and horizontal gradients of latent image, the joint likelihood is given by -

`$$L(\boldsymbol{\theta}) = L_h(\boldsymbol{\theta})\times L_v(\boldsymbol{\theta}) = f_{\theta}(|Y_{h,\omega}|^2,\forall \omega) \times f_{\theta}(|Y_{v,\omega'}|^2,\forall \omega')$$`

--

* Our objective is to find `\(\hat{\theta} = \underset{\boldsymbol{\theta}}{\text{argmax}} \ \ logL(\boldsymbol{\boldsymbol{\theta}}) = \underset{\theta}{\text{argmax}} \ \ logL_h(\boldsymbol{\theta}) + logL_v(\boldsymbol{\theta})\)`

---

# Challenges in ML Estimation

* The first task is to find maximizer of `\(L(\boldsymbol{\theta})\)`. 

--

* The parameter `\(\boldsymbol{\theta}\)` is involved in the expression `\(\lambda_{\omega}\)` through `\(|K_{\omega}|^2\)`, which itself is a complicated function of parameter.

--

* Before we start using any optimization technique, we should empirically investigate the behavior of `\(L(\boldsymbol{\theta})\)` as a function of `\(\boldsymbol{\theta}\)`.

* Simulated experiments using disc kernel is conducted for this purpose.

--

* Sequence of values for `\(r \in [1,4]\)` with `\(\Delta{r} = 0.05\)`, and for `\(\sigma \in [0.01,0.4]\)` with `\(\Delta{\sigma} = 0.01\)` are considered, with `\(\eta = 0.001\)` constant.

---

## Experiment - 1

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="exp1.png" alt="Figure: (a) 101 x 101 Sharp Image, (b) Blurred Image Using disc kernel with r = 3, (c) Disc Kernel with r = 3, (d) Levelplot of log likelihood as a function of sigma and r" width="70%" /&gt;
&lt;p class="caption"&gt;Figure: (a) 101 x 101 Sharp Image, (b) Blurred Image Using disc kernel with r = 3, (c) Disc Kernel with r = 3, (d) Levelplot of log likelihood as a function of sigma and r&lt;/p&gt;
&lt;/div&gt;


---

## Experiment - 2

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="exp2.png" alt="Figure: (a) 101 x 101 Sharp Image, (b) Blurred Image Using disc kernel with r = 2.5, (c) Disc Kernel with r = 2.5, (d) Levelplot of log likelihood as a function of sigma and r" width="72%" /&gt;
&lt;p class="caption"&gt;Figure: (a) 101 x 101 Sharp Image, (b) Blurred Image Using disc kernel with r = 2.5, (c) Disc Kernel with r = 2.5, (d) Levelplot of log likelihood as a function of sigma and r&lt;/p&gt;
&lt;/div&gt;

---

# Challenges in ML Estimation

* The first task is to find maximizer of `\(L(\boldsymbol{\theta})\)`. 

* The parameter `\(\boldsymbol{\theta}\)` is involved in the expression `\(\lambda_{\omega}\)` through `\(|K_{\omega}|^2\)`, which itself is a complicated function of parameter.

* Before we start using any optimization technique, we should empirically investigate the behavior of `\(L(\boldsymbol{\theta})\)` as a function of `\(\boldsymbol{\theta}\)`.

* Simulated experiments using disc kernel is conducted for this purpose.

* Sequence of values for `\(r \in [1,4]\)` with `\(\Delta{r} = 0.05\)`, and for `\(\sigma \in [0.01,0.4]\)` with `\(\Delta{\sigma} = 0.01\)` are considered, with `\(\eta = 0.001\)` constant.

* Global maxima don't always correspond to the actual parameters of the blur kernel.

--

* We aim for nearly accurate estimation of blur kernel parameters.

--

* Proper choice of `\(\sigma\)` is required (A common situation is *Bayesian paradigm* !).

--

* Simulations can be used to find a reasonable value of `\(\sigma\)`.


---

## Experiment: Choice of `\(\sigma\)`

* `\(\sigma = 0.2\)` seems to be a reasonable choice.

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="empstud.png" alt="Figure: Empirical study to find sigma" width="100%" /&gt;
&lt;p class="caption"&gt;Figure: Empirical study to find sigma&lt;/p&gt;
&lt;/div&gt;

---

# Deblurring

* An important application of depth estimation is post-capture refocusing of blurred areas.

--

* For that we need efficient deblurring algorithms.

--

* 
--

*



---



---
















    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
